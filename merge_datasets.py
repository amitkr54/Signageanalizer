"""
Merge VR_AI + safety datasets for fine-tuning existing CubiCasa model
This adds fire safety detection to your trained floor plan model
"""

import os
import shutil
from pathlib import Path
import yaml

# ========================================
# CONFIGURATION
# ========================================

# Fire safety dataset paths
VR_AI_PATH = "C:/Users/Admin/Downloads/VR_AI.v1-for-floorplan.yolov8"
SAFETY_PATH = "C:/Users/Admin/Downloads/safety.v1-for-floorplan.yolov8"
TEXT_DATASET_PATH = "C:/Users/Admin/Downloads/floorplan_text.v1.yolov8" # UPDATE THIS with your text dataset

# Output merged dataset path
OUTPUT_DIR = "C:/Users/Admin/Downloads/fire_safety_merged"

# ========================================
# MERGE SCRIPT
# ========================================

def merge_fire_safety_datasets():
    """Merge VR_AI and safety datasets"""
    
    print("üî• Merging Fire Safety Datasets...")
    
    # Create output directories
    output_path = Path(OUTPUT_DIR)
    for split in ['train', 'valid']:
        (output_path / split / 'images').mkdir(parents=True, exist_ok=True)
        (output_path / split / 'labels').mkdir(parents=True, exist_ok=True)
    
    total_images = 0
    all_classes = set()
    
    # Merge both datasets
    for dataset_name, dataset_path in [
        ("vr_ai", VR_AI_PATH), 
        ("safety", SAFETY_PATH),
        ("text", TEXT_DATASET_PATH) # Added for hybrid OCR
    ]:
        print(f"\nüì¶ Processing {dataset_name}...")
        
        if not os.path.exists(dataset_path):
            print(f"   ‚ö†Ô∏è  Path not found: {dataset_path}")
            continue
        
        source_path = Path(dataset_path)
        
        # Copy images and labels for both train and valid
        for split in ['train', 'valid']:
            src_img_dir = source_path / split / 'images'
            src_lbl_dir = source_path / split / 'labels'
            
            if not src_img_dir.exists():
                print(f"   ‚ö†Ô∏è  {split}/images not found, skipping...")
                continue
            
            dst_img_dir = output_path / split / 'images'
            dst_lbl_dir = output_path / split / 'labels'
            
            img_count = 0
            # Process .jpg and .png files
            for ext in ['*.jpg', '*.jpeg', '*.png']:
                for img_file in src_img_dir.glob(ext):
                    # Prefix with dataset name to avoid conflicts
                    new_img_name = f"{dataset_name}_{img_file.name}"
                    shutil.copy2(img_file, dst_img_dir / new_img_name)
                    
                    # Copy corresponding label file
                    lbl_file = src_lbl_dir / f"{img_file.stem}.txt"
                    if lbl_file.exists():
                        new_lbl_name = f"{dataset_name}_{img_file.stem}.txt"
                        shutil.copy2(lbl_file, dst_lbl_dir / new_lbl_name)
                        
                        # Collect class IDs from labels
                        with open(lbl_file, 'r') as f:
                            for line in f:
                                class_id = line.strip().split()[0]
                                all_classes.add(int(class_id))
                    
                    img_count += 1
            
            print(f"   ‚úÖ {split}: {img_count} images")
            total_images += img_count
    
    # Create data.yaml
    create_data_yaml(output_path, all_classes)
    
    print(f"\nüéâ Merge Complete!")
    print(f"üìÅ Output: {OUTPUT_DIR}")
    print(f"üìä Total images: {total_images}")
    print(f"\n‚úÖ Ready for fine-tuning in Colab!")

def create_data_yaml(output_path, class_ids):
    """Create data.yaml for fire safety dataset"""
    
    # Fire safety class names (adjust based on your datasets)
    class_names = [
        'wall', 'door', 'window', 'stairs', 'exit_sign', 
        'fire_extinguisher', 'fire_alarm', 'emergency_exit', 
        'fire_hose_reel', 'fire_hydrant', 'aed', 'text'
    ]
    
    yaml_content = f"""# Fire Safety Merged Dataset
# VR_AI + safety datasets

path: .
train: train/images
val: valid/images

nc: {len(class_names)}
names: {class_names}
"""
    
    with open(output_path / 'data.yaml', 'w') as f:
        f.write(yaml_content)
    
    print(f"\n   ‚úÖ Created data.yaml with {len(class_names)} fire safety classes")

if __name__ == "__main__":
    merge_fire_safety_datasets()
    print("\nüìã Next Steps:")
    print("1. Zip the fire_safety_merged folder")
    print("2. Upload to Google Drive")
    print("3. In Colab, fine-tune your CubiCasa model with this data")
